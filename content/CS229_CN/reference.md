---
title: 参考文献
---
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences, 116(32):15849–15854, 2019. [🔗](https://www.pnas.org/doi/abs/10.1073/pnas.1903070116) ^belkin2019reconciling

Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. SIAM Journal on Mathematics of Data Science, 2(4):1167–1180, 2020. [🔗](https://epubs.siam.org/doi/abs/10.1137/20M1336072) ^belkin2020two

David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. *Journal of the American Statistical Association*, 112(518):859–877, 2017. [🔗](https://www.tandfonline.com/doi/abs/10.1080/01621459.2017.1285773) ^blei2017variational

Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. *arXiv preprint arXiv:2108.07258*, 2021. [🔗](https://arxiv.org/abs/2108.07258) ^bommasani2021oppo

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. *Advances in neural information processing systems*, 33:1877–1901, 2020. [🔗](https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html) ^brown2020lang

Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In *International Conference on Machine Learning*, pages 1597–1607. PMLR, 2020. [🔗](http://proceedings.mlr.press/v119/chen20j.html) ^chen2020simple

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages 4171–4186, 2019. [🔗](https://aclanthology.org/N19-1423) ^devlin2019bert

Jeff Z HaoChen, Colin Wei, Jason D Lee, and Tengyu Ma. Shape matters: Understanding the implicit bias of the noise covariance. In *Proceedings of  Conference on Learning Theory*, pages 2315--2357, 2021. [🔗](https://proceedings.mlr.press/v125/woodworth20a) ^haochen2021shape

Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-dimensional ridgeless least squares interpolation. *Annals of statistic*s, 50(2):949, 2022. [🔗](https://pmc.ncbi.nlm.nih.gov/articles/PMC9481183/) ^hastie2022surprises

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 770–778, 2016. [🔗](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) ^he2016resnet

Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. An introduction to statistical learning, volume 112. Springer, 2021. [🔗](https://link.springer.com/book/10.1007/978-1-0716-1418-1) ^james2021intro

Sergey Ioffe and Szegedy Christian. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In *International conference on machine learning*, pages 448-456, 2015. [🔗](https://proceedings.mlr.press/v37/ioffe15.html) ^sergey2015bn

Yuxin Wu and Kaiming He. Group normalization. In *Proceedings of the European conference on computer vision*, pages 3-19, 2018. [🔗](https://openaccess.thecvf.com/content_ECCV_2018/html/Yuxin_Wu_Group_Normalization_ECCV_2018_paper.html) ^yuxin2018gn

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. *arXiv preprint arXiv:1412.6980*, 2014. [🔗](https://arxiv.org/abs/1412.6980v9) ^kingma2014adam

Diederik P Kingma and Max Welling. Auto-encoding variational bayes. *arXiv preprint arXiv:1312.6114*, 2013. [🔗](https://arxiv.org/abs/1312.6114) ^kingma2013auto

Song Mei, and Andrea Montanari. The generalization error of random features regression: Precise asymptotics and the double descent curve. *Communications on Pure and Applied Mathematics*, 75(4):667–766, 2022. [🔗](https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.22008) ^mei2022gen

Preetum Nakkiran. More data can hurt for linear regression: Sample-wise double descent. 2019. [🔗](https://arxiv.org/abs/1912.07242) ^nakkiran2019more

Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma. Optimal regularization can mitigate double descent. 2020. [🔗](https://arxiv.org/abs/2003.01897) ^nakkiran2020opt

Manfred Opper. Statistical mechanics of learning: Generalization, pages 922–925, 1995. [🔗](https://philpapers.org/rec/oppsmo) ^opper1995stats

Manfred Opper. Learning to generalize. *Frontiers of Life*, 3(part 2):763–775, 2001. ^opper2001learning

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. *Advances in Neural Information Processing Systems*, 30, 2017. [🔗](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) ^vaswani2017attention

Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In *Proceedings of  Conference on Learning Theory*, pages 3635--3673, 2020. [🔗](https://proceedings.mlr.press/v125/woodworth20a) ^woodworth2020kernel



