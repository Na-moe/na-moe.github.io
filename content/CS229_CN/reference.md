---
title: 参考文献
---
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences, 116(32):15849–15854, 2019. [🔗](https://www.pnas.org/doi/abs/10.1073/pnas.1903070116) ^belkin2019reconciling

Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. SIAM Journal on Mathematics of Data Science, 2(4):1167–1180, 2020. [🔗](https://epubs.siam.org/doi/abs/10.1137/20M1336072) ^belkin2020two

David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. *Journal of the American Statistical Association*, 112(518):859–877, 2017. [🔗](https://www.tandfonline.com/doi/abs/10.1080/01621459.2017.1285773) ^Blei2017variational

Jeff Z HaoChen, Colin Wei, Jason D Lee, and Tengyu Ma. Shape matters: Understanding the implicit bias of the noise covariance. In *Proceedings of  Conference on Learning Theory*, pages 2315--2357, 2021. [🔗](https://proceedings.mlr.press/v125/woodworth20a) ^haochen2021shape

Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-dimensional ridgeless least squares interpolation. *Annals of statistic*s, 50(2):949, 2022. [🔗](https://pmc.ncbi.nlm.nih.gov/articles/PMC9481183/) ^hastie2022surprises

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 770–778, 2016. [🔗](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) ^he2016resnet

Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. An introduction to statistical learning, volume 112. Springer, 2021. [🔗](https://link.springer.com/book/10.1007/978-1-0716-1418-1) ^james2021intro

Sergey Ioffe and Szegedy Christian. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In *International conference on machine learning*, pages 448-456, 2015. [🔗](https://proceedings.mlr.press/v37/ioffe15.html) ^sergey2015bn

Yuxin Wu and Kaiming He. Group normalization. In *Proceedings of the European conference on computer vision*, pages 3-19, 2018. [🔗](https://openaccess.thecvf.com/content_ECCV_2018/html/Yuxin_Wu_Group_Normalization_ECCV_2018_paper.html) ^yuxin2018gn

Diederik P Kingma and Max Welling. Auto-encoding variational bayes. *arXiv preprint arXiv:1312.6114*, 2013. [🔗](https://arxiv.org/abs/1312.6114) ^kingma2013auto

Song Mei, and Andrea Montanari. The generalization error of random features regression: Precise asymptotics and the double descent curve. *Communications on Pure and Applied Mathematics*, 75(4):667–766, 2022. [🔗](https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.22008) ^mei2022gen

Preetum Nakkiran. More data can hurt for linear regression: Sample-wise double descent. 2019. [🔗](https://arxiv.org/abs/1912.07242) ^nakkiran2019more

Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma. Optimal regularization can mitigate double descent. 2020. [🔗](https://arxiv.org/abs/2003.01897) ^nakkiran2020opt

Manfred Opper. Statistical mechanics of learning: Generalization, pages 922–925, 1995. [🔗](https://philpapers.org/rec/oppsmo) ^opper1995stats

Manfred Opper. Learning to generalize. *Frontiers of Life*, 3(part 2):763–775, 2001. ^opper2001learning

Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In *Proceedings of  Conference on Learning Theory*, pages 3635--3673, 2020. [🔗](https://proceedings.mlr.press/v125/woodworth20a) ^woodworth2020kernel



