---
title: 监督学习
---
此习题集中习题 (1-5) 为 [2020-夏](https://cs229.stanford.edu/summer2020/ps1.pdf) 版的习题集#1

注意：解答中不能使用机器学习专用库 (如 `scikit-learn`)

所涉及的数据及初始代码：[下载](https://cs229.stanford.edu/summer2020/ps1.zip)

---

### 1. \[40 分\] 线性分类器 (逻辑回归与高斯判别分析)

本题涵盖了目前课程中涉及的两种概率性线性分类器。第一种是判别式线性分类器：逻辑回归。第二种是生成式线性分类器：高斯判别分析 (GDA)。这两种算法均通过寻找线性决策边界将数据划分为两个类别，但基于不同的假设。本题旨在帮助深入理解这两种算法的异同及其优缺点。

本题将使用以下文件中的两个数据集及初始代码：

- `src/linearclass/ds1_{train,valid}.csv`
- `src/linearclass/ds2_{train,valid}.csv`
- `src/linearclass/logreg.py`
- `src/linearclass/gda.py`

每个文件包含 $n$ 个样本，每行一个样本 $(x^{(i)}, y^{(i)})$. 具体而言，第 $i$ 行包含 $x_1^{(i)} \in \mathbb{R}, x_2^{(i)} \in \mathbb{R}$ 和 $y^{(i)} \in \{0,1\}$ 三列。在接下来的子问题中，我们将研究如何使用逻辑回归和高斯判别分析（GDA）对这两个数据集进行二元分类。

(a) \[10 分\]

课程中我们学习了逻辑回归的平均经验损失：

$$
J(\theta) = -\frac{1}{n} \sum_{i=1}^n \left( y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right),
$$

其中 $y^{(i)} \in \{0,1\}$, $h_\theta(x) = g(\theta^T x)$, 且 $g(z) = 1/(1 + e^{-z})$.

求该函数的 Hessian 矩阵 $H$, 并证明对于任意向量 $z$, 均有 $z^T H z \geq 0$.

**提示**：可先证明 $\sum_i \sum_j z_i x_i x_j z_j = (x^T z)^2 \geq 0$. 同时注意 $g'(z) = g(z)(1 - g(z))$.

**备注**：这是证明矩阵 $H$ 半正定 (记为 $H \succeq 0$) 的标准方法之一。由此可推出 $J$ 是凸函数，且除全局最小值外不存在局部最小值。若采用其他方法证明 $H \succeq 0$, 也可使用。

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#1(a)]]

(b) \[5 分\] 编程题

根据 `src/linearclass/logreg.py` 中的指导，使用牛顿法训练逻辑回归分类器。从 $\theta = 0$ 开始运行牛顿法，直至 $\theta$ 的更新量足够小：具体而言，训练至第一次满足 $\|\theta_k - \theta_{k-1}\|_1 < \epsilon$ 的迭代 $k$, 其中 $\epsilon = 1 \times 10^{-5}$. 确保将模型在验证集上的预测概率写入代码指定的文件中。

绘制验证数据的散点图，横轴为 $x_1$, 纵轴为 $x_2$. 为区分两个类别，使用不同标记表示 $y^{(i)} = 0$ 和 $y^{(i)} = 1$ 的样本。在同一图中，绘制逻辑回归得到的决策边界 (即对应 $p(y|x) = 0.5$ 的直线)。

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#1(b)]]

(c) \[5 分\] 回顾在高斯判别分析中，我们通过以下方程对 $(x, y)$ 的联合分布进行建模：

$$
\begin{aligned}
	p(y) &= \begin{cases}
		\phi & \text{若 } y = 1 \\
		1 - \phi & \text{若 } y = 0
	\end{cases} \\
	p(x|y=0) &= \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu_0)^T \Sigma^{-1} (x - \mu_0)\right) \\
	p(x|y=1) &= \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu_1)^T \Sigma^{-1} (x - \mu_1)\right),
\end{aligned}
$$

其中 $\phi, \mu_0, \mu_1$ 和 $\Sigma$ 是模型的参数。

假设我们已经拟合了参数 $\phi, \mu_0, \mu_1$ 和 $\Sigma$, 现在需要给定新数据点 $x$ 预测 $y$. 为了证明 GDA 得到的分类器具有线性决策边界，请证明后验分布可以表示为：

$$
p(y=1 \mid x; \phi, \mu_0, \mu_1, \Sigma) = \frac{1}{1 + \exp(-(\theta^T x + \theta_0))},
$$

其中 $\theta \in \mathbb{R}^d$ 和 $\theta_0 \in \mathbb{R}$ 是 $\phi, \mu_0, \mu_1$ 和 $\Sigma$ 的适当函数。

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#1(c)]]

(d) \[7 分\] 对于给定数据集，我们声称参数的最大似然估计由以下公式给出：

$$
\begin{aligned}
	\hat{\phi} &= \frac{1}{n} \sum_{i=1}^n \mathbf{1}\{y^{(i)} = 1\} \\
	\hat{\mu}_0 &= \frac{\sum_{i=1}^n \mathbf{1}\{y^{(i)} = 0\} x^{(i)}}{\sum_{i=1}^n \mathbf{1}\{y^{(i)} = 0\}} \\
	\hat{\mu}_1 &= \frac{\sum_{i=1}^n \mathbf{1}\{y^{(i)} = 1\} x^{(i)}}{\sum_{i=1}^n \mathbf{1}\{y^{(i)} = 1\}} \\
	\hat{\Sigma} &= \frac{1}{n} \sum_{i=1}^n (x^{(i)} - \mu_{y^{(i)}})(x^{(i)} - \mu_{y^{(i)}})^T
\end{aligned}
$$

数据的对数似然函数为：

$$
\begin{aligned}
	\ell(\phi, \mu_0, \mu_1, \Sigma) 
		&= \log \prod_{i=1}^n p(x^{(i)}, y^{(i)}; \phi, \mu_0, \mu_1, \Sigma) \\
		&= \log \prod_{i=1}^n p(x^{(i)} | y^{(i)}; \mu_0, \mu_1, \Sigma) p(y^{(i)}; \phi).
\end{aligned}
$$

通过最大化 $\ell$ 对四个参数的取值，证明 $\phi, \mu_0, \mu_1$ 和 $\Sigma$ 的最大似然估计确实由以上公式给出。(可假设数据集中至少存在一个正例和一个负例，以确保 $\mu_0$ 和 $\mu_1$ 定义中的分母非零。)

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#1(d)]]

(e) \[5 分\] 编程题

在 `src/linearclass/gda.py` 中填写代码以计算 $\phi, \mu_0, \mu_1$ 和 $\Sigma$, 使用这些参数推导 $\theta$, 并利用得到的 GDA 模型对验证集进行预测。确保将模型在验证集上的预测结果写入代码指定的文件中。

绘制**验证数据**的散点图，横轴为 $x_1$, 纵轴为 $x_2$. 为区分两个类别，使用不同标记表示 $y^{(i)} = 0$ 和 $y^{(i)} = 1$ 的样本。在同一图中，绘制 GDA 得到的决策边界 (即对应 $p(y|x) = 0.5$ 的直线)。

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#1(e)]]

(f) \[2 分\] 对于数据集 1，比较在 (b) 和 (e) 中分别通过逻辑回归和 GDA 得到的验证集图，并用几句话简要评述你的观察结果。

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#1(f)]]

(g) \[5 分\] 对数据集 2 重复 (b) 和 (e) 的步骤。在数据集 2 的**验证集**上创建类似的图，并将这些图包含在你的报告中。

在哪个数据集上 GDA 的表现似乎比逻辑回归差？这可能是什么原因？

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#1(g)]]

(h) \[1 分\] 对于在 (f) 和 (g) 中 GDA 表现较差的数据集，你能否找到一种对 $x^{(i)}$ 的变换，使得 GDA 的表现显著改善？这种变换可能是什么？

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#1(h)]]

---

### 2. \[30 分\] 不完整的、仅含正类标签的数据

在本问题中，我们将探讨在无法获取完整标签的情况下训练二分类器的问题。具体而言，我们考虑一个在现实场景中并不少见的情况：我们只能获取部分正例的标签，而所有负例和其余正例均无标签。

我们将该场景形式化如下：令 $\{(x^{(i)}, t^{(i)})\}_{i=1}^n$ 为一个独立同分布样本的标准数据集，其中 $x^{(i)}$ 表示输入特征，$t^{(i)}$ 表示真实标签。现假设我们无法观测到 $t^{(i)}$，而只能观测到部分正例的标签。具体而言，我们假设观测到的 $y^{(i)}$ 由以下过程生成：

$$
\begin{aligned}
	&\forall x,\ p(y^{(i)} = 1 \mid t^{(i)} = 1, x^{(i)} = x) = \alpha, \\
	&\forall x,\ p(y^{(i)} = 0 \mid t^{(i)} = 1, x^{(i)} = x) = 1 - \alpha \\
	&\forall x,\ p(y^{(i)} = 1 \mid t^{(i)} = 0, x^{(i)} = x) = 0, \\
	&\forall x,\ p(y^{(i)} = 0 \mid t^{(i)} = 0, x^{(i)} = x) = 1
\end{aligned}
$$

其中 $\alpha \in (0,1)$ 是一个未知标量。换言之，若未观测到的“真实”标签 $t^{(i)}$ 为 $1$，我们以 $\alpha$ 的概率观测到标签 $y^{(i)} = 1$; 若未观测到的“真实”标签 $t^{(i)} = 0$, 我们总是观测到标签 $y^{(i)} = 0$.

本问题的最终目标是仅基于部分标签 $y$ 构建一个对真实标签 $t$ 的二分类器 $h$. 换言之，我们希望构建 $h$，使得仅使用 $x$ 和 $y$ 时，$h(x^{(i)}) \approx p(t^{(i)} = 1 \mid x^{(i)})$ 尽可能接近真实概率。

*现实示例：假设我们维护了一个参与跨膜信号传导的蛋白质数据库。数据库中每个样本均参与信号传导过程，但许多参与跨膜信号传导的蛋白质未被纳入数据库。训练一个分类器来识别应添加到数据库的蛋白质将非常有用。用我们的符号表示：每个样本 $x^{(i)}$ 对应一个蛋白质，若该蛋白质在数据库中则 $y^{(i)} = 1$, 否则为 0; 若该蛋白质参与跨膜信号传导过程 (即应被添加到数据库) 则 $t^{(i)} = 1$, 否则为 0.*

在后续问题中，我们将使用以下文件提供的数据集和初始代码：
- `src/posonly/{train,valid,test}.csv`
- `src/posonly/posonly.py`

每个文件包含以下列：$x_1, x_2, y$ 和 $t$. 与问题1相同，每行对应一个样本。$y^{(i)}$ 由上述过程生成，其中 $\alpha$ 为未知参数。

(a) \[5分\] **编程题：理想 (完全观测) 情况**

首先我们考虑一个假设性 (但无实际意义) 的情况：在训练时能够获取真实标签 $t$. 在 `src/posonly/posonly.py` 中编写一个使用 $x_1$ 和 $x_2$ 作为输入特征的逻辑回归分类器，并使用 $t$ 标签进行训练。此部分忽略 $y$ 标签。将训练好的模型在**测试集**上的预测结果输出到代码指定文件中。

创建一个可视化测试集的散点图：横轴为 $x_1$, 纵轴为 $x_2$. 使用不同符号区分真实标签 $t^{(i)} = 1$ 和 $t^{(i)} = 0$ 的样本。在同一图中，用红色绘制模型得到的决策边界 (即模型预测概率等于0.5的等高线)。将该图纳入提交报告中。

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#2(a)]]

(b) \[5分\] **编程题：基于部分标签的朴素方法**

我们现在考虑无法获取真实标签 $t$ 的情况，训练时只能使用部分标签 $y$. 扩展 `src/posonly/posonly.py` 中的代码，重新训练分类器 (仍使用 $x_1$ 和 $x_2$ 作为输入特征，但仅使用 $y$ 标签)。将模型在**测试集**上的预测结果输出到指定文件 (详见代码注释)。

创建一个测试集可视化散点图：横轴为 $x_1$, 纵轴为 $x_2$. 对真实标签 $t^{(i)} = 1$ 和 $t^{(i)} = 0$ 的样本使用不同符号标注 (尽管训练时仅使用 $y^{(i)}$ 标签，但绘图时使用真实标签 $t^{(i)}$ )。在同一图中，用红色绘制模型得到的决策边界 (即模型预测概率等于0.5的等高线)。将该图纳入提交报告中。

注意：算法应学习一个能近似预测概率 $p(y^{(i)} = 1 \mid x^{(i)})$ 的函数 $h(\cdot)$. 同时注意，该模型在预测目标概率 $p(t^{(i)} = 1 \mid x^{(i)})$ 时预期表现较差。

在以下子问题中，我们将尝试在仅能获取部分观测数据 (即仅能使用 $\{(x^{(i)}, y^{(i)})\}_{i=1}^n$ ) 的情况下解决该问题，并尝试预测 $p(t^{(i)} = 1 \mid x^{(i)})$.

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#2(b)]]

(c) \[5分\] **贝叶斯定理热身**

证明在我们的假设下，对于任意样本 $i$：

$$
p(t^{(i)} = 1 \mid y^{(i)} = 1, x^{(i)}) = 1 \tag{1}
$$

即观测到部分正标签 $y^{(i)} = 1$ 可确定隐藏真实标签为 $1$. 需使用贝叶斯定理推导 (非形式化解释不得分)。

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#2(c)]]

(d) \[5分\] 证明对于任意样本，真实标签 $t^{(i)}$ 为正的概率是部分标签为正概率的 $1/\alpha$ 倍。即证明：

^eqps1-2
$$
p(t^{(i)} = 1 \mid x^{(i)}) = \frac{1}{\alpha} \cdot p(y^{(i)} = 1 \mid x^{(i)}) \tag{2}
$$

注意：上述等式表明，若已知 $\alpha$ 值，则可通过乘以 $1/\alpha$ 将近似预测 $h(x^{(i)}) \approx p(y^{(i)} = 1 \mid x^{(i)})$ 的函数 $h(\cdot)$ 转换为近似预测 $p(t^{(i)} = 1 \mid x^{(i)})$ 的函数。

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#2(d)]]

(e) \[5分\] **估计 $\alpha$**

前述解决方案需要已知 $\alpha$ 值，但我们实际并不知道。现在我们将基于近似预测 $p(y^{(i)} = 1 \mid x^{(i)})$ 的函数 $h(\cdot)$ (由 (b) 获得) 设计一种估计 $\alpha$ 的方法。

为简化分析，假设我们通过某种方法获得了能完美预测 $p(y^{(i)} = 1 \mid x^{(i)})$ 的函数 $h(x)$, 即 $h(x^{(i)}) = p(y^{(i)} = 1 \mid x^{(i)})$. 我们有关键假设 $p(t^{(i)} = 1 \mid x^{(i)}) \in \{0,1\}$, 该假设意味着“真实”标签 $t^{(i)}$ 的生成过程是无噪声的。注意，我们并非假设观测标签 $y^{(i)}$ 无噪声 (这种假设是不合理的)。

现在我们将证明：

^eqps1-3
$$
\alpha = \mathbb{E}[h(x^{(i)}) \mid y^{(i)} = 1] \tag{3}
$$

为证明此式，需证明当 $y^{(i)} = 1$ 时 $h(x^{(i)}) = \alpha$, 当 $y^{(i)} = 0$ 时 $h(x^{(i)}) = 0$.

上述结果启发了以下通过样本估计 $\alpha$ 的算法：令 $V_+$ 为验证集 $V$ 中带标签 (即正例) 的样本集合，即 $V_+ = \{x^{(i)} \in V \mid y^{(i)} = 1\}$.

我们使用

$$
\alpha \approx \frac{1}{|V_+|} \sum_{x^{(i)} \in V_+} h(x^{(i)})
$$

来估计 $\alpha$. (下一小题将要求实现该算法。本小题仅需证明式 [[ps1_supervised_learning#^eqps1-3|(3)]]，且难度略高于其他小题。)

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#2(e)]]

(f) \[5分\] **编程题**

使用验证集，通过对验证集中所有带标签样本的预测值取平均来估计常数 $\alpha$：[^1]

$$
\alpha \approx \frac{1}{|V_+|} \sum_{x^{(i)} \in V_+} h(x^{(i)}).
$$

在 `src/posonly/posonly.py` 中添加代码，根据 (d) 中得到的式 [[ps1_supervised_learning#^eqps1-2|(2)]] 并使用估计的 $\alpha$ 值，对 (b) 中所得分类器的预测值 $h(y^{(i)} = 1 \mid x^{(i)})$ 进行重新缩放。

最后，创建测试集可视化散点图：横轴为 $x_1$, 纵轴为 $x_2$. 对真实标签 $t^{(i)} = 1$ 和 $t^{(i)} = 0$ 的样本使用不同符号标注 (尽管训练时仅使用 $y^{(i)}$ 标签，但绘图时使用真实标签 $t^{(i)}$ )。在同一图中，用红色绘制模型得到的决策边界 (即模型**调整后**预测概率等于 $0.5$ 的等高线)。将该图纳入提交报告中。

**备注**：我们发现真实概率 $p(t \mid x)$ 与 $p(y \mid x)$ 仅相差一个常数因子。这意味着，如果我们的任务仅是对样本进行排序 (例如按参与跨膜信号传导的可能性对蛋白质进行排序)，则实际上无需估计 $\alpha$. 基于 $p(y \mid x)$ 的排序结果与基于 $p(t \mid x)$ 的排序结果一致。

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#2(f)]]

---

### 3. \[25 分\] 泊松回归

在本问题中，我们将构建另一种常用GLM模型，称为泊松回归。在广义线性模型 (GLM) 中，指数族分布的选择取决于待解决问题的类型。若处理分类问题，则采用支持离散类别的指数族分布 (例如伯努利分布或范畴分布)。类似地，若输出值为连续实数，可采用高斯分布或拉普拉斯分布 (二者均属指数族)。当需要预测计数型数据时 (例如根据输入特征预测每日预期邮件接收量，或未来一小时内预计进店顾客数等)，需使用支持整数域 (即计数) 的概率分布。泊松分布不仅满足这一条件，而且恰属于指数族分布。

以下将通过子问题逐步完成：首先证明泊松分布属于指数族，推导假设函数形式，建立模型训练更新规则，最终基于给定数据集训练实际模型并进行测试集预测。

(a) \[5 分\] 考虑参数为 $\lambda$ 的泊松分布：

$$
p(y; \lambda) = \frac{e^{-\lambda} \lambda^y}{y!}
$$

(其中 $y$ 取正整数，$y!$ 表示 $y$ 的阶乘。) 证明该泊松分布属于指数族分布，并明确给出 $b(y)$, $\eta$, $T(y)$ 和 $a(\eta)$ 的表达式。

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#3(a)]]

(b) \[3 分\] 考虑使用泊松响应变量构建 GLM 回归模型，该分布的典型响应函数为何？ (可利用参数为 $\lambda$ 的泊松随机变量其均值为 $\lambda$ 的特性。)

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#3(b)]]

(c) \[7 分\] 对于训练集 $\{(x^{(i)}, y^{(i)}) : i = 1, \dots, n\}$, 设单个样本的对数似然为 $\log p(y^{(i)} | x^{(i)}; \theta)$. 通过计算对数似然对 $\theta_j$ 的偏导数，推导使用泊松响应变量和典型响应函数的 GLM 模型随机梯度上升更新规则。

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#3(c)]]

(d) \[10 分\] **编程题**

某网站需预测其日访问量。网站管理者已收集历史流量数据及相关特征，这些特征被认为对预测每日访客数具有价值。数据集被划分为训练集/验证集，初始代码位于以下文件：

- `src/poisson/{train,valid}.csv`
- `src/poisson/poisson.py`

我们将采用泊松回归建模每日访客数。需注意，泊松回归的应用前提是数据服从泊松分布，且其自然参数为输入特征的线性组合 (即 $\eta = \theta^T x$ )。请在 `src/poisson/poisson.py` 中实现泊松回归模型，并采用**全批量梯度上升法**最大化 $\theta$ 的对数似然。设置停止准则为参数变化的范数小于阈值 (例如 $10^{-5}$)。

使用训练完成的模型对**验证集**进行期望值预测，并绘制真实计数值与预测计数值的散点图 (基于验证集)。图中横轴为真实计数值，纵轴为对应的预测期望值。需注意，真实计数值为整数，而预测期望值通常为连续实数。

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#3(d)]]

---

### 4. \[15 分\] 广义线性模型的凸性

本问题将探讨广义线性模型 (GLM) 基于指数族分布建模输出变量时所具备的优良性质，重点分析其凸性特性。

通常，GLM 训练采用负对数似然 (NLL) 作为损失函数，该方式在数学上等价于极大似然估计 (即最大化对数似然等价于最小化负对数似然)。本文旨在证明 GLM 的 NLL 损失函数关于模型参数是**凸函数**。值得注意的是，凸函数具有局部极小值即为全局极小值的特性，且现有大量研究关注如何通过梯度下降或随机梯度下降等算法高效优化各类凸函数。

首先回顾指数族分布的概率密度函数形式：

$$
p(y; \eta) = b(y) \exp(\eta^T T(y) - a(\eta)),
$$

其中 $\eta$ 为分布的*自然参数*。在广义线性模型中，$\eta$ 被建模为 $\theta^T x$, 这里 $x \in \mathbb{R}^d$ 表示样本输入特征，$\theta \in \mathbb{R}^d$ 为可学习参数。为证明 GLM 的 NLL 损失具有凸性，我们将其分解为若干子问题逐步分析。核心思路是证明损失函数关于模型参数的二阶导数 (即 Hessian 矩阵) 在任意参数取值下均为半正定矩阵。在此过程中，我们将首先展示指数族分布的若干重要性质。

为简化推导，我们限定 $\eta$ 为标量情形。假设 $p(Y|X; \theta) \sim \text{ExponentialFamily}(\eta)$，其中 $\eta \in \mathbb{R}$ 为标量，且 $T(y) = y$. 此时指数族分布可表示为：

$$
p(y; \eta) = b(y) \exp(\eta y - a(\eta)).
$$

(a) \[5 分\] 推导该分布的均值表达式，证明 $\mathbb{E}[Y; \eta] = \frac{\partial}{\partial \eta} a(\eta)$ (注意：由 $\eta = \theta^T x$ 可得 $\mathbb{E}[Y; \eta] = \mathbb{E}[Y|X; \theta]$ ). 换言之，证明指数族分布的均值是对数配分函数对自然参数的一阶导数。

**提示**：可从 $\frac{\partial}{\partial \eta} \int p(y; \eta) dy = \int \frac{\partial}{\partial \eta} p(y; \eta) dy$ 着手推导。

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#4(a)]]

(b) \[5 分\] 推导该分布的方差表达式。特别地，证明 $\text{Var}(Y; \eta) = \frac{\partial^2}{\partial \eta^2} a(\eta)$ (同样注意 $\text{Var}(Y; \eta) = \text{Var}(Y|X; \theta)$)。换言之，证明指数族分布的方差是对数配分函数对自然参数的二阶导数。

**提示**：基于前序结论可简化推导过程。

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#4(b)]]

(c) \[5 分\] 将损失函数 $\ell(\theta)$ (即分布的 NLL) 表示为 $\theta$ 的函数，计算损失关于 $\theta$ 的 Hessian 矩阵，并证明该矩阵恒为半正定。由此完成 GLM 的 NLL 损失函数凸性证明。

**提示 1**：结合链式法则与前序结论可简化推导。

**提示 2**：注意任意概率分布的方差均具有非负性。

**结论要点**：
- 所有 GLM 模型均关于其模型参数具有凸性
- 指数族概率分布具备优良数学特性：一般而言计算分布的均值和方差需进行复杂积分运算，但对于指数族分布， 可通过求导运算简便求得。

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#4(c)]]

---

### 5. \[25 分\] 线性回归：何为线性？

在前两个问题中，我们探讨了如何用数据的线性函数解决回归问题。本题将研究如何通过特征映射，利用线性回归来拟合数据的非线性函数，并分析其局限性 (后续课程将讨论改进方案)。

(a) \[5 分\] **学习输入的三次多项式**

假设存在数据集 $\{(x^{(i)}, y^{(i)})\}_{i=1}^n$, 其中 $x^{(i)}, y^{(i)} \in \mathbb{R}$. 我们需要拟合三次多项式 $h_\theta(x) = \theta_3 x^3 + \theta_2 x^2 + \theta_1 x^1 + \theta_0$. 核心在于：尽管该函数关于输入 $x$ 非线性，但关于未知参数 $\theta$ 保持线性，这使我们可以通过以下方式将其转化为线性回归问题。

定义特征映射 $\phi : \mathbb{R} \to \mathbb{R}^4$，将原始输入 $x$ 转换为四维向量：

$$
\phi(x) = \begin{bmatrix} 1 \\ x \\ x^2 \\ x^3 \end{bmatrix} \in \mathbb{R}^4 \tag{4}
$$

令 $\hat{x} \in \mathbb{R}^4$ 表示 $\phi(x)$ 的简写，$\hat{x}^{(i)} \triangleq \phi(x^{(i)})$ 表示训练集中转换后的输入。通过将原始输入 $x^{(i)}$ 替换为 $\hat{x}^{(i)}$, 我们构建新数据集 $\{(\hat{x}^{(i)}, y^{(i)})\}_{i=1}^n$. 此时，将 $h_\theta(x) = \theta_3 x^3 + \theta_2 x^2 + \theta_1 x^1 + \theta_0$ 拟合至原数据集，等价于将线性函数 $h_\theta(\hat{x}) = \theta_3 \hat{x}_3 + \theta_2 \hat{x}_2 + \theta_1 \hat{x}_1 + \theta_0 = \theta^T \hat{x}$ 拟合至新数据集，因为：

$$
h_\theta(x) = \theta_3 x^3 + \theta_2 x^2 + \theta_1 x^1 + \theta_0 = \theta_3 \phi(x)_3 + \theta_2 \phi(x)_2 + \theta_1 \phi(x)_1 + \theta_0 = \theta^T \hat{x} \tag{5}
$$

换言之，我们可通过新数据集的线性回归求解参数 $\theta_0, \dots, \theta_3$.

请写出：1) 新数据集 $\{(\hat{x}^{(i)}, y^{(i)})\}_{i=1}^n$ 上线性回归的目标函数 $J(\theta)$；2) 该数据集上批量梯度下降算法的更新规则。

**术语说明**：在机器学习中，$\phi$ 常称为*特征映射*，用于将原始输入 $x$ 映射到新变量集。为区分这两类变量，我们称 $x$ 为*输入属性*，称 $\phi(x)$ 为*特征* (注：不同文献术语存在差异，本课程将统一遵循上述约定)。

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#5(a)]]

(b) \[5 分\] **编程问题：三次多项式回归**

本问题使用以下文件中的数据集：
- `src/featuremaps/{train,valid,test}.csv`

每个文件包含两列：$x$ 和 $y$. 根据前述术语，$x$ 为属性 (本例中为一维)，$y$ 为输出标签。

基于前文推导，使用三次多项式特征映射，通过**正规方程法**实现线性回归。请在 `src/featuremaps/featuremap.py` 中提供的初始代码基础上实现算法。

绘制训练数据的散点图，并将学习得到的假设函数以平滑曲线形式叠加于图上。将该图纳入报告作为本题答案。

*备注:* 假设 $\hat{X}$ 是转换后的数据集的设计矩阵。你可能会遇到 $\hat{X}^T\hat{X}$ 不可逆的情况。为了代码实现的数值稳定性，总是使用 `np.linalg.solve` 来直接得到参数，而非显式计算逆并乘以 $\hat{X}^Ty$.

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#5(b)]]

(c) \[5 分\] **编程题：$k$ 次多项式回归**

现在我们将上述思路推广到 $k$ 次多项式，考虑特征映射 $\phi : \mathbb{R} \to \mathbb{R}^{k+1}$ 定义为：

$$
\phi(x) = \begin{bmatrix} 1 \\ x \\ x^2 \\ \vdots \\ x^k \end{bmatrix} \in \mathbb{R}^{k+1} \tag{6}
$$

按照前一子问题的相同流程，实现 $k = 3, 5, 10, 20$ 时的算法。创建与前一子问题类似的图形，并用不同颜色绘制每个 $k$ 值对应的假设曲线。在图中包含图例以标明各颜色对应的k值。

在报告中提交该图作为本子问题的解答。观察随着 $k$ 值的增加，训练数据拟合效果的变化，并在图中简要评论你的观察结果。

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#5(c)]]

(d) \[5 分\] **编程题：其他特征映射**

你可能已经观察到，需要相对较高的次数 $k$ 才能很好地拟合给定的训练数据，这是因为低次多项式无法很好地解释 (即近似) 该数据集。通过数据可视化，你可能已经发现y可以用正弦波很好地近似。事实上，我们通过从 $y = \sin(x) + \xi$ 中采样生成该数据，其中 $\xi$ 是高斯分布的噪声。请更新特征映射 $\phi$ 以包含正弦变换：

$$
\phi(x) = \begin{bmatrix} 1 \\ x \\ x^2 \\ \vdots \\ x^k \\ \sin(x) \end{bmatrix} \in \mathbb{R}^{k+2} \tag{7}
$$

使用更新后的特征映射，对 $k = 0, 1, 2, 3, 5, 10, 20$ 这些值训练不同的模型，并像之前一样在数据上绘制得到的假设曲线。

提交该图作为本子问题的解答。与前一子问题的拟合模型进行比较，并简要评论使用此特征映射后拟合效果的显著差异。

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#5(d)]]

(e) \[5 分\] 表达能力强的模型与小数据集的过拟合问题

对于问题的剩余部分，我们将考虑一个更小的数据集 (你目前使用的数据集的随机子集)，该数据集包含的样本数量要少得多，文件如下：

- `src/featuremaps/small.csv`

我们将探讨当特征数量开始超过训练集中样本数量时会发生什么情况。在这个小数据集上使用以下特征映射运行你的算法：

$$
\phi(x) = \begin{bmatrix} 1 \\ x \\ x^2 \\ \vdots \\ x^k \end{bmatrix} \in \mathbb{R}^{k+1} \tag{8}
$$

其中 $k = 1, 2, 5, 10, 20$。

创建各种假设曲线的图形 (与之前的子问题类似)。观察随着 $k$ 值的增加，训练数据拟合效果的变化。在报告中提交该图并评论你的观察结果。

**备注**：你观察到的模型开始非常拟合训练数据但突然"失控"的现象是由**过拟合**引起的。目前的直观理解是，当你的数据量相对于可能模型族 (即假设类，在这种情况下是所有k次多项式的族) 的表达能力较小时，会导致过拟合。

粗略地说，假设函数集合"非常灵活"，特别容易以不自然的方式强制通过所有数据点。换句话说，模型解释了训练数据集中的噪声，而这些噪声本不应该被解释。这会损害模型在测试样本上的预测能力。我们将在后续讲解学习理论和偏差-方差权衡的课程中更详细地描述过拟合。

> [!example]- 答案  
>  ![[CS229_CN/Problem_Set/problem_set_1/solution#5(e)]]

[^1]: 使用验证集而非训练集估计 $\alpha$ 存在特定原因。但为本问题之目的，我们暂不讨论这一细微差别，无需理解二者区别。
