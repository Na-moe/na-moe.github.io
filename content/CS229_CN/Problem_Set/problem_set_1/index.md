---
title: "习题集 #1: 监督学习"
---
此为 [2020-夏](https://cs229.stanford.edu/summer2020/ps1.pdf) 版的习题集#1

注意：解答中不能使用机器学习专用库 (如 `scikit-learn`)

所涉及的数据及初始代码：[下载]()

---

### 1. \[40 分\] 线性分类器 (逻辑回归与高斯判别分析)

本题涵盖了目前课程中涉及的两种概率性线性分类器。第一种是判别式线性分类器：逻辑回归。第二种是生成式线性分类器：高斯判别分析 (GDA)。这两种算法均通过寻找线性决策边界将数据划分为两个类别，但基于不同的假设。本题旨在帮助深入理解这两种算法的异同及其优缺点。

本题将使用以下文件中的两个数据集及初始代码：

- `src/linearclass/ds1_{train,valid}.csv`
- `src/linearclass/ds2_{train,valid}.csv`
- `src/linearclass/logreg.py`
- `src/linearclass/gda.py`

每个文件包含 $n$ 个样本，每行一个样本 $(x^{(i)}, y^{(i)})$. 具体而言，第 $i$ 行包含 $x_1^{(i)} \in \mathbb{R}, x_2^{(i)} \in \mathbb{R}$ 和 $y^{(i)} \in \{0,1\}$ 三列。在接下来的子问题中，我们将研究如何使用逻辑回归和高斯判别分析（GDA）对这两个数据集进行二元分类。

(a) \[10 分\]

课程中我们学习了逻辑回归的平均经验损失：

$$
J(\theta) = -\frac{1}{n} \sum_{i=1}^n \left( y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right),
$$

其中 $y^{(i)} \in \{0,1\}$, $h_\theta(x) = g(\theta^T x)$, 且 $g(z) = 1/(1 + e^{-z})$.

求该函数的 Hessian 矩阵 $H$, 并证明对于任意向量 $z$, 均有 $z^T H z \geq 0$.

**提示**：可先证明 $\sum_i \sum_j z_i x_i x_j z_j = (x^T z)^2 \geq 0$. 同时注意 $g'(z) = g(z)(1 - g(z))$.

**备注**：这是证明矩阵 $H$ 半正定 (记为 $H \succeq 0$) 的标准方法之一。由此可推出 $J$ 是凸函数，且除全局最小值外不存在局部最小值。若采用其他方法证明 $H \succeq 0$, 也可使用。

(b) \[5 分\] 编程题

根据 `src/linearclass/logreg.py` 中的指导，使用牛顿法训练逻辑回归分类器。从 $\theta = 0$ 开始运行牛顿法，直至 $\theta$ 的更新量足够小：具体而言，训练至第一次满足 $\|\theta_k - \theta_{k-1}\|_1 < \epsilon$ 的迭代 $k$, 其中 $\epsilon = 1 \times 10^{-5}$. 确保将模型在验证集上的预测概率写入代码指定的文件中。

绘制验证数据的散点图，横轴为 $x_1$, 纵轴为 $x_2$. 为区分两个类别，使用不同标记表示 $y^{(i)} = 0$ 和 $y^{(i)} = 1$ 的样本。在同一图中，绘制逻辑回归得到的决策边界 (即对应 $p(y|x) = 0.5$ 的直线)。

(c) \[5 分\] 回顾在高斯判别分析中，我们通过以下方程对 $(x, y)$ 的联合分布进行建模：

$$
\begin{aligned}
	p(y) &= \begin{cases}
		\phi & \text{若 } y = 1 \\
		1 - \phi & \text{若 } y = 0
	\end{cases} \\
	p(x|y=0) &= \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu_0)^T \Sigma^{-1} (x - \mu_0)\right) \\
	p(x|y=1) &= \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu_1)^T \Sigma^{-1} (x - \mu_1)\right),
\end{aligned}
$$

其中 $\phi, \mu_0, \mu_1$ 和 $\Sigma$ 是模型的参数。

假设我们已经拟合了参数 $\phi, \mu_0, \mu_1$ 和 $\Sigma$, 现在需要给定新数据点 $x$ 预测 $y$. 为了证明 GDA 得到的分类器具有线性决策边界，请证明后验分布可以表示为：

$$
p(y=1 \mid x; \phi, \mu_0, \mu_1, \Sigma) = \frac{1}{1 + \exp(-(\theta^T x + \theta_0))},
$$

其中 $\theta \in \mathbb{R}^d$ 和 $\theta_0 \in \mathbb{R}$ 是 $\phi, \mu_0, \mu_1$ 和 $\Sigma$ 的适当函数。

(d) \[7 分\] 对于给定数据集，我们声称参数的最大似然估计由以下公式给出：

$$
\begin{aligned}
	\hat{\phi} &= \frac{1}{n} \sum_{i=1}^n \mathbf{1}\{y^{(i)} = 1\} \\
	\hat{\mu}_0 &= \frac{\sum_{i=1}^n \mathbf{1}\{y^{(i)} = 0\} x^{(i)}}{\sum_{i=1}^n \mathbf{1}\{y^{(i)} = 0\}} \\
	\hat{\mu}_1 &= \frac{\sum_{i=1}^n \mathbf{1}\{y^{(i)} = 1\} x^{(i)}}{\sum_{i=1}^n \mathbf{1}\{y^{(i)} = 1\}} \\
	\hat{\Sigma} &= \frac{1}{n} \sum_{i=1}^n (x^{(i)} - \mu_{y^{(i)}})(x^{(i)} - \mu_{y^{(i)}})^T
\end{aligned}
$$

数据的对数似然函数为：

$$
\begin{aligned}
	\ell(\phi, \mu_0, \mu_1, \Sigma) 
		&= \log \prod_{i=1}^n p(x^{(i)}, y^{(i)}; \phi, \mu_0, \mu_1, \Sigma) \\
		&= \log \prod_{i=1}^n p(x^{(i)} | y^{(i)}; \mu_0, \mu_1, \Sigma) p(y^{(i)}; \phi).
\end{aligned}
$$

通过最大化 $\ell$ 对四个参数的取值，证明 $\phi, \mu_0, \mu_1$ 和 $\Sigma$ 的最大似然估计确实由以上公式给出。(可假设数据集中至少存在一个正例和一个负例，以确保 $\mu_0$ 和 $\mu_1$ 定义中的分母非零。)

(e) \[5 分\] 编程题

在 `src/linearclass/gda.py` 中填写代码以计算 $\phi, \mu_0, \mu_1$ 和 $\Sigma$, 使用这些参数推导 $\theta$, 并利用得到的 GDA 模型对验证集进行预测。确保将模型在验证集上的预测结果写入代码指定的文件中。

绘制**验证数据**的散点图，横轴为 $x_1$, 纵轴为 $x_2$. 为区分两个类别，使用不同标记表示 $y^{(i)} = 0$ 和 $y^{(i)} = 1$ 的样本。在同一图中，绘制 GDA 得到的决策边界 (即对应 $p(y|x) = 0.5$ 的直线)。

(f) \[2 分\] 对于数据集 1，比较在 (b) 和 (e) 中分别通过逻辑回归和 GDA 得到的验证集图，并用几句话简要评述你的观察结果。

(g) \[5 分\] 对数据集 2 重复 (b) 和 (e) 的步骤。在数据集 2 的**验证集**上创建类似的图，并将这些图包含在你的报告中。

在哪个数据集上 GDA 的表现似乎比逻辑回归差？这可能是什么原因？

(h) \[1 分\] 对于在 (f) 和 (g) 中 GDA 表现较差的数据集，你能否找到一种对 $x^{(i)}$ 的变换，使得 GDA 的表现显著改善？这种变换可能是什么？

> [!example]- 答案  
>   ![[CS229_CN/Problem_Set/problem_set_1/solution#1. 线性分类器 (逻辑回归与高斯判别分析)]]

---

### 2. \[30 分\] 不完整的、仅含正类标签的数据

在本问题中，我们将探讨在无法获取完整标签的情况下训练二分类器的问题。具体而言，我们考虑一个在现实场景中并不少见的情况：我们只能获取部分正例的标签，而所有负例和其余正例均无标签。

我们将该场景形式化如下：令 $\{(x^{(i)}, t^{(i)})\}_{i=1}^n$ 为一个独立同分布样本的标准数据集，其中 $x^{(i)}$ 表示输入特征，$t^{(i)}$ 表示真实标签。现假设我们无法观测到 $t^{(i)}$，而只能观测到部分正例的标签。具体而言，我们假设观测到的 $y^{(i)}$ 由以下过程生成：

$$
\begin{aligned}
	&\forall x,\ p(y^{(i)} = 1 \mid t^{(i)} = 1, x^{(i)} = x) = \alpha, \\
	&\forall x,\ p(y^{(i)} = 0 \mid t^{(i)} = 1, x^{(i)} = x) = 1 - \alpha \\
	&\forall x,\ p(y^{(i)} = 1 \mid t^{(i)} = 0, x^{(i)} = x) = 0, \\
	&\forall x,\ p(y^{(i)} = 0 \mid t^{(i)} = 0, x^{(i)} = x) = 1
\end{aligned}
$$

其中 $\alpha \in (0,1)$ 是一个未知标量。换言之，若未观测到的“真实”标签 $t^{(i)}$ 为 $1$，我们以 $\alpha$ 的概率观测到标签 $y^{(i)} = 1$; 若未观测到的“真实”标签 $t^{(i)} = 0$, 我们总是观测到标签 $y^{(i)} = 0$.

本问题的最终目标是仅基于部分标签 $y$ 构建一个对真实标签 $t$ 的二分类器 $h$. 换言之，我们希望构建 $h$，使得仅使用 $x$ 和 $y$ 时，$h(x^{(i)}) \approx p(t^{(i)} = 1 \mid x^{(i)})$ 尽可能接近真实概率。

*现实示例：假设我们维护了一个参与跨膜信号传导的蛋白质数据库。数据库中每个样本均参与信号传导过程，但许多参与跨膜信号传导的蛋白质未被纳入数据库。训练一个分类器来识别应添加到数据库的蛋白质将非常有用。用我们的符号表示：每个样本 $x^{(i)}$ 对应一个蛋白质，若该蛋白质在数据库中则 $y^{(i)} = 1$, 否则为 0; 若该蛋白质参与跨膜信号传导过程 (即应被添加到数据库) 则 $t^{(i)} = 1$, 否则为 0.*

在后续问题中，我们将使用以下文件提供的数据集和初始代码：
- `src/posonly/{train,valid,test}.csv`
- `src/posonly/posonly.py`

每个文件包含以下列：$x_1, x_2, y$ 和 $t$. 与问题1相同，每行对应一个样本。$y^{(i)}$ 由上述过程生成，其中 $\alpha$ 为未知参数。

(a) \[5分\] **编程题：理想 (完全观测) 情况**

首先我们考虑一个假设性 (但无实际意义) 的情况：在训练时能够获取真实标签 $t$. 在 `src/posonly/posonly.py` 中编写一个使用 $x_1$ 和 $x_2$ 作为输入特征的逻辑回归分类器，并使用 $t$ 标签进行训练。此部分忽略 $y$ 标签。将训练好的模型在**测试集**上的预测结果输出到代码指定文件中。

创建一个可视化测试集的散点图：横轴为 $x_1$, 纵轴为 $x_2$. 使用不同符号区分真实标签 $t^{(i)} = 1$ 和 $t^{(i)} = 0$ 的样本。在同一图中，用红色绘制模型得到的决策边界 (即模型预测概率等于0.5的等高线)。将该图纳入提交报告中。

(b) \[5分\] **编程题：基于部分标签的朴素方法**

我们现在考虑无法获取真实标签 $t$ 的情况，训练时只能使用部分标签 $y$. 扩展 `src/posonly/posonly.py` 中的代码，重新训练分类器 (仍使用 $x_1$ 和 $x_2$ 作为输入特征，但仅使用 $y$ 标签)。将模型在**测试集**上的预测结果输出到指定文件 (详见代码注释)。

创建一个测试集可视化散点图：横轴为 $x_1$, 纵轴为 $x_2$. 对真实标签 $t^{(i)} = 1$ 和 $t^{(i)} = 0$ 的样本使用不同符号标注 (尽管训练时仅使用 $y^{(i)}$ 标签，但绘图时使用真实标签 $t^{(i)}$ )。在同一图中，用红色绘制模型得到的决策边界 (即模型预测概率等于0.5的等高线)。将该图纳入提交报告中。

注意：算法应学习一个能近似预测概率 $p(y^{(i)} = 1 \mid x^{(i)})$ 的函数 $h(\cdot)$. 同时注意，该模型在预测目标概率 $p(t^{(i)} = 1 \mid x^{(i)})$ 时预期表现较差。

在以下子问题中，我们将尝试在仅能获取部分观测数据 (即仅能使用 $\{(x^{(i)}, y^{(i)})\}_{i=1}^n$ ) 的情况下解决该问题，并尝试预测 $p(t^{(i)} = 1 \mid x^{(i)})$.

(c) \[5分\] **贝叶斯定理热身**

证明在我们的假设下，对于任意样本 $i$：

$$
p(t^{(i)} = 1 \mid y^{(i)} = 1, x^{(i)}) = 1 \tag{1}
$$

即观测到部分正标签 $y^{(i)} = 1$ 可确定隐藏真实标签为 $1$. 需使用贝叶斯定理推导 (非形式化解释不得分)。

(d) \[5分\] 证明对于任意样本，真实标签 $t^{(i)}$ 为正的概率是部分标签为正概率的 $1/\alpha$ 倍。即证明：

^eqps1-2
$$
p(t^{(i)} = 1 \mid x^{(i)}) = \frac{1}{\alpha} \cdot p(y^{(i)} = 1 \mid x^{(i)}) \tag{2}
$$

注意：上述等式表明，若已知 $\alpha$ 值，则可通过乘以 $1/\alpha$ 将近似预测 $h(x^{(i)}) \approx p(y^{(i)} = 1 \mid x^{(i)})$ 的函数 $h(\cdot)$ 转换为近似预测 $p(t^{(i)} = 1 \mid x^{(i)})$ 的函数。

(e) \[5分\] **估计 $\alpha$**

前述解决方案需要已知 $\alpha$ 值，但我们实际并不知道。现在我们将基于近似预测 $p(y^{(i)} = 1 \mid x^{(i)})$ 的函数 $h(\cdot)$ (由 (b) 获得) 设计一种估计 $\alpha$ 的方法。

为简化分析，假设我们通过某种方法获得了能完美预测 $p(y^{(i)} = 1 \mid x^{(i)})$ 的函数 $h(x)$, 即 $h(x^{(i)}) = p(y^{(i)} = 1 \mid x^{(i)})$. 我们有关键假设 $p(t^{(i)} = 1 \mid x^{(i)}) \in \{0,1\}$, 该假设意味着“真实”标签 $t^{(i)}$ 的生成过程是无噪声的。注意，我们并非假设观测标签 $y^{(i)}$ 无噪声 (这种假设是不合理的)。

现在我们将证明：

^eqps1-3
$$
\alpha = \mathbb{E}[h(x^{(i)}) \mid y^{(i)} = 1] \tag{3}
$$

为证明此式，需证明当 $y^{(i)} = 1$ 时 $h(x^{(i)}) = \alpha$, 当 $y^{(i)} = 0$ 时 $h(x^{(i)}) = 0$.

上述结果启发了以下通过样本估计 $\alpha$ 的算法：令 $V_+$ 为验证集 $V$ 中带标签 (即正例) 的样本集合，即 $V_+ = \{x^{(i)} \in V \mid y^{(i)} = 1\}$.

我们使用

$$
\alpha \approx \frac{1}{|V_+|} \sum_{x^{(i)} \in V_+} h(x^{(i)})
$$

来估计 $\alpha$. (下一小题将要求实现该算法。本小题仅需证明式 [[CS229_CN/Problem_Set/problem_set_1/index#^eqps1-3|(3)]]，且难度略高于其他小题。)

(f) \[5分\] **编程题**

使用验证集，通过对验证集中所有带标签样本的预测值取平均来估计常数 $\alpha$：[^1]

$$
\alpha \approx \frac{1}{|V_+|} \sum_{x^{(i)} \in V_+} h(x^{(i)}).
$$

在 `src/posonly/posonly.py` 中添加代码，根据 (d) 中得到的式 [[CS229_CN/Problem_Set/problem_set_1/index#^eqps1-2|(2)]] 并使用估计的 $\alpha$ 值，对 (b) 中所得分类器的预测值 $h(y^{(i)} = 1 \mid x^{(i)})$ 进行重新缩放。

最后，创建测试集可视化散点图：横轴为 $x_1$, 纵轴为 $x_2$. 对真实标签 $t^{(i)} = 1$ 和 $t^{(i)} = 0$ 的样本使用不同符号标注 (尽管训练时仅使用 $y^{(i)}$ 标签，但绘图时使用真实标签 $t^{(i)}$ )。在同一图中，用红色绘制模型得到的决策边界 (即模型**调整后**预测概率等于 $0.5$ 的等高线)。将该图纳入提交报告中。

**备注**：我们发现真实概率 $p(t \mid x)$ 与 $p(y \mid x)$ 仅相差一个常数因子。这意味着，如果我们的任务仅是对样本进行排序 (例如按参与跨膜信号传导的可能性对蛋白质进行排序)，则实际上无需估计 $\alpha$. 基于 $p(y \mid x)$ 的排序结果与基于 $p(t \mid x)$ 的排序结果一致。

---

### 3. \[25 分\] 特征向量、特征值与谱定理

$n \times n$ 矩阵 $A \in \mathbb{R}^{n \times n}$ 的特征值是其特征多项式 $p_A(\lambda) = \det(\lambda I - A)$ 的根 (通常可能为复数)。特征值也可定义为满足 $A x = \lambda x$ (其中 $x \in \mathbb{C}^n$) 的标量 $\lambda \in \mathbb{C}$. 我们称这样的二元组 $(x, \lambda)$ 为特征向量-特征值对。在本问题中，我们使用符号 $\operatorname{diag}(\lambda_1, \dots, \lambda_n)$ 表示对角元素为 $\lambda_1, \dots, \lambda_n$ 的对角矩阵，即

$$
\operatorname{diag}(\lambda_1, \dots, \lambda_n) = 
\begin{bmatrix}
\lambda_1 & 0 & 0 & \cdots & 0 \\
0 & \lambda_2 & 0 & \cdots & 0 \\
0 & 0 & \lambda_3 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & \lambda_n
\end{bmatrix}.
$$

(a) 假设矩阵 $A \in \mathbb{R}^{n \times n}$ 可对角化，即存在可逆矩阵 $T \in \mathbb{R}^{n \times n}$ 和对角矩阵 $\Lambda = \operatorname{diag}(\lambda_1, \dots, \lambda_n)$, 使得 $A = T \Lambda T^{-1}$. 记 $t^{(i)}$ 为 $T$ 的列向量，即 $T = [t^{(1)} \cdots t^{(n)}]$, 其中 $t^{(i)} \in \mathbb{R}^n$. 证明 $A t^{(i)} = \lambda_i t^{(i)}$, 从而说明 $A$ 的*特征值/特征向量*对为 $(t^{(i)}, \lambda_i)$.

若矩阵 $U \in \mathbb{R}^{n \times n}$ 满足 $U^T U = I$, 则称其为正交矩阵。**谱定理** (可能是线性代数中最重要的定理之一) 指出：若 $A \in \mathbb{R}^{n \times n}$ 是对称矩阵 (即 $A = A^T$ ), 则 $A$ 可*通过实正交矩阵对角化*。即，存在对角矩阵 $\Lambda \in \mathbb{R}^{n \times n}$ 和正交矩阵 $U \in \mathbb{R}^{n \times n}$, 使得 $U^T A U = \Lambda$, 或等价地，

$$
A = U \Lambda U^T.
$$

记 $\lambda_i = \lambda_i(A)$ 为 $A$ 的第 $i$ 个特征值。

(b) 设 $A$ 为对称矩阵。证明：若 $U = [u^{(1)} \cdots u^{(n)}]$ 为正交矩阵 (其中 $u^{(i)} \in \mathbb{R}^n$), 且 $A = U \Lambda U^T$, 则 $u^{(i)}$ 是 $A$ 的特征向量，且满足 $A u^{(i)} = \lambda_i u^{(i)}$, 其中 $\Lambda = \operatorname{diag}(\lambda_1, \dots, \lambda_n)$.

(c) 证明：若 $A$ 是半正定矩阵，则对每个 $i$, 有 $\lambda_i(A) \geq 0$.

[^1]: 使用验证集而非训练集估计 $\alpha$ 存在特定原因。但为本问题之目的，我们暂不讨论这一细微差别，无需理解二者区别。
