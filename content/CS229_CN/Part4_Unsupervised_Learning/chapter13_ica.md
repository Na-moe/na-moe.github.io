---
title: 第 13 章 独立成分分析
---
| [[chapter12_pca\|上一章]] | [[CS229_CN/index#目录\|目录]] | [[chapter14_self-supervised_learning_and_foundation_models\|下一章]] |
| :--------------------: | :-----------------------: | :---------------------------------------------------------------: |

下一个主题是独立成分分析 (Independent Components Analysis, ICA)。与 PCA 类似，ICA 也旨在找到一组新的基来表示数据，但其目标非常不同。

作为一个启发性例子，考虑「鸡尾酒会问题」。假设有 $d$ 位说话者在派对上同时说话，房间里的任何一个麦克风都只记录了这 $d$ 位说话者声音的叠加组合。但是假设在房间里放置了 $d$ 个不同的麦克风，并且由于每个麦克风与每个说话者的距离不同，它记录了说话者声音的不同组合。使用这些麦克风记录，能否分离出原始的 $d$ 位说话者的语音信号？

为了形式化这个问题，假设存在一些数据 $s \in \mathbb{R}^d$, 这些数据是由 $d$ 个独立的源生成的。我们观察到的是

$$
x = As,
$$

其中 $A$ 是一个未知的方阵，称为 **混合矩阵 (mixing matrix)**。一系列观察结果构成了数据集 $\{x^{(i)}; i = 1, \dots, n\}$, 我们的目标是恢复生成源数据 $s^{(i)}$ (其中 $x^{(i)} = As^{(i)}$).

在鸡尾酒会问题中，$s^{(i)}$ 是一个 $d$ 维向量，$s_j^{(i)}$ 是说话者 $j$ 在时刻 $i$ 发出的声音。此外，$x^{(i)}$ 是一个 $d$ 维向量，$x_j^{(i)}$ 是麦克风 $j$ 在时刻 $i$ 记录的声学读数。

令 $W = A^{-1}$ 为 **解混矩阵 (unmixing matrix)**。我们的目标是找到 $W$, 以便给定麦克风记录 $x^{(i)}$, 可以通过计算 $s^{(i)} = W x^{(i)}$ 来恢复源数据。为了方便记号，也令 $w_i^T$ 表示 $W$ 的第 $i$ 行，以便

$$
W = 
    \begin{bmatrix} 
        \ -w_1^T- \ \\
        \ -\vdots- \ \\
        \ -w_d^T- \
    \end{bmatrix}.
$$

因此，$w_i \in \mathbb{R}^d$, 并且第 $j$ 个源可以恢复为 $s_j^{(i)} = w_j^T x^{(i)}$.

## 13.1 ICA 的不确定性

$W=A^{-1}$ 可以在多大程度上被恢复？如果我们对源和混合矩阵一无所知，很容易看出在仅给定 $x^{(i)}$ 的情况下，存在一些固有的 $A$ 中无法恢复的不确定性。

具体来说，令 $P$ 为任意 $d \times d$ 置换矩阵。这意味着 $P$ 的每一行和每一列都恰好有一个 “$1$”. 以下是一些置换矩阵的例子：

$$
P = \begin{bmatrix} 
        \ 0 & 1 & 0 \ \\
        \ 1 & 0 & 0 \ \\
        \ 0 & 0 & 1 \
    \end{bmatrix}; 
    \quad P = \begin{bmatrix} 
        \ 0 & 1 \ \\
        \ 1 & 0 \ 
    \end{bmatrix}; 
    \quad P = \begin{bmatrix}
        \ 1 & 0 \ \\
        \ 0 & 1 \ 
    \end{bmatrix}.
$$

如果 $z$ 是一个向量，那么 $Pz$ 是另一个向量，它包含 $z$ 的坐标的置换版本。仅给定 $x^{(i)}$是无法区分 $W$ 和 $PW$ 的。具体来说，原始源的置换是不确定的，这不足为奇。幸运的是，这对于大多数应用来说并不重要。

此外，也无法恢复 $w_i$ 的正确比例。例如，如果 $A$ 被 $2A$ 替换，并且每个 $s^{(i)}$ 都被 $(0.5)s^{(i)}$ 替换，那么我们观察到的 $x^{(i)} = 2A \cdot (0.5)s^{(i)}$ 仍然相同。进一步地，如果 $A$ 的某一列被因子 $\alpha$ 缩放，而相应的源被因子 $1/\alpha$ 缩放，那么仅给定 $x^{(i)}$, 就无法确定发生了这种情况。因此，无法恢复源的「正确」比例。然而，对于我们关心的应用——包括鸡尾酒会问题——这种不确定性同样不重要。具体来说，将说话者 $j$ 的语音信号 $s_j^{(i)}$ 乘以某个正因子 $\alpha$ 只会影响该语音的音量。符号变化也不重要，$s_j^{(i)}$ 和 $-s_j^{(i)}$ 在扬声器上播放时听起来是相同的。因此，如果算法找到的 $w_i$ 被任何非零实数缩放，相应的恢复源 $s_i = w_i^T x$ 也将缩放同样的因子；但这通常不重要。(这些评论也适用于我们在课堂上讨论的脑/MEG 数据的 ICA)

这些是 ICA 中唯一的不确定性来源吗？事实证明如果源 $s_i$ 是*非高斯*分布的，这些就确实是仅有的不确定性来源。为了了解高斯数据的困难所在，考虑一个例子，其中 $n=2$ 且 $s \sim N(0, I)$. 这里，$I$ 是 2x2 单位矩阵。注意，标准正态分布 $N(0, I)$ 的密度等高线是中心位于原点的圆，并且密度是旋转对称的。

现在，假设观察到一些 $x = As$, 其中 $A$ 是混合矩阵。那么 $x$ 的分布将是高斯分布，即有 $x \sim N(0, AA^T)$, 因为

$$
\begin{gathered}
\mathbb{E}_{s \sim N(0, I)}[x] = \mathbb{E}[As] = A\mathbb{E}[s] = 0 \\
\text{Cov}[x] = \mathbb{E}_{s \sim N(0, I)}[xx^T] = \mathbb{E}[Ass^T A^T] = A\mathbb{E}[ss^T]A^T = A \cdot \text{Cov}[s] \cdot A^T = AA^T
\end{gathered}
$$

现在，令 $R$ 为任意正交矩阵 (不太正式地说，为旋转或反射矩阵)，使得 $RR^T = R^T R = I$, 并令 $A' = AR$. 那么，如果数据是按照 $A'$ 而不是 $A$ 混合的，那么我们将观察到 $x' = A's$. $x'$ 的分布也是高斯分布，$x' \sim N(0, AA^T)$, 因为

$$
\mathbb{E}_{s \sim N(0, I)}[x'(x')^T] = \mathbb{E}[A'ss^T (A')^T] = \mathbb{E}[ARss^T (AR)^T] = ARR^T A^T = AA^T.
$$

因此，无论混合矩阵是 $A$ 还是 $A'$, 我们都会观察到服从 $N(0, AA^T)$ 分布的数据。因此，无法判断源是使用 $A$ 还是 $A'$ 混合的。混合矩阵中存在一个任意的旋转分量，无法从数据中确定，并且无法恢复原始源。

上面的论证是基于多元标准正态分布是旋转对称的事实。尽管高斯数据的 ICA 前景黯淡，但事实证明，只要数据不是高斯分布的，就有可能在给定足够数据的情况下恢复 $d$ 个独立源。

## 13.2 密度与线性变换

在继续推导 ICA 算法之前，先简要介绍一下线性变换对概率密度的影响。

假设随机变量 $s$ 服从密度函数 $p_s(s)$. 为简单起见，暂时假设 $s \in \mathbb{R}$ 是一个实数。现在，令随机变量 $x$ 定义为 $x = As$ (这里 $x \in \mathbb{R}, A \in \mathbb{R}$). 令 $p_x$ 为 $x$ 的密度。那么 $p_x$ 是什么？

令 $W = A^{-1}$。计算特定值 $x$ 的「概率」时，很容易想到计算 $s = Wx$, 然后评估 $p_s$ 在该点的值，并得出结论 $p_x(x) = p_s(Wx)$. 然而，这是不正确的。例如，令 $s \sim \text{Uniform}[0, 1]$，所以 $p_s(s) = 1\{0 \le s \le 1\}$. 现在，令 $A = 2$, 所以 $x = 2s$. 显然，$x$ 在区间 $[0, 2]$ 上均匀分布。因此，其密度为 $p_x(x) = (0.5)1\{0 \le x \le 2\}$. 这不等于 $p_s(Wx)$, 其中 $W = 0.5 = A^{-1}$. 正确的公式是 $p_x(x) = p_s(Wx)|W|$.

更一般地，如果 $s$ 是一个具有密度 $p_s$ 的向量值分布，并且 $x = As$ 对于一个方阵、可逆矩阵 $A$，那么 $x$ 的密度由下式给出：

$$
p_x(x) = p_s(Wx) \cdot |W|,
$$

其中 $W = A^{-1}$.

**备注.**  如果你见过矩阵 $A$ 将 $[0, 1]^d$ 映射到一个体积为 $|A|$ 的集合的结果，那么这里有一个另一种记忆上述 $p_x$ 公式的方法，它也推广了我们之前的一维例子。具体来说，令 $A \in \mathbb{R}^{d \times d}$, 并且像往常一样令 $W = A^{-1}$. 也令 $C_1 = [0, 1]^d$ 为 $d$ 维超立方体，并定义 $C_2 = \{As : s \in C_1\} \subseteq \mathbb{R}^d$ 为 $C_1$ 在由 $A$ 给定的映射下的像。那么，这是一个线性代数中的标准结果（实际上，也是定义行列式的一种方式），即 $C_2$ 的体积由 $|A|$ 给出。现在，假设 $s$ 在 $[0, 1]^d$ 上均匀分布，所以其密度为 $p_s(s) = 1\{s \in C_1\}$. 那么显然 $x$ 将在 $C_2$ 中均匀分布。因此，可以找到其密度为 $p_x(x) = 1\{x \in C_2\}/\text{vol}(C_2)$ (因为在 $C_2$ 上的积分必须为 $1$)。但是利用矩阵逆的行列式就是行列式的倒数这一事实，我们有 $1/\text{vol}(C_2) = 1/|A| = |A^{-1}| = |W|$. 因此，$p_x(x) = 1\{x \in C_2\}|W| = 1\{Wx \in C_1\}|W| = p_s(Wx)|W|$.

## 13.3 ICA 算法

现在准备推导 ICA 算法，一个由 Bell 和 Sejnowski 提出的算法，并将其解释为最大似然估计方法。(这与他们原始的解释不同，后者涉及一个复杂的概念，称为 infomax 原理，鉴于对 ICA 的现代理解，这已不再必要。)

假设每个源 $s_j$ 的分布由密度 $p_s$ 给出，并且源 $s$ 的联合分布由下式给出：

$$
p(s) = \prod_{j=1}^d p_s(s_j).
$$

注意，通过将联合分布建模为边缘分布的乘积，满足了源是独立的假设。利用上一节的公式，得到 $x = As = W^{-1}s$ 的密度如下：

$$
p(x) = \prod_{j=1}^d p_s(w_j^T x) \cdot |W|.
$$

剩下的就是指定各个源的密度 $p_s$.

回想一下，给定一个实值随机变量 $z$, 其累积分布函数 (cumulative distribution function, cdf) $F$ 定义为 $F(z_0) = P(z \le z_0) = \int_{-\infty}^{z_0} p_z(z)dz$, 并且密度 $p_z(z) = F'(z)$ 是累积分布函数的导数。

因此，要指定 $s_i$ 的密度，只需要为其指定一个累积分布函数。累积分布函数必须是一个从零单调递增到一的函数。根据之前的讨论，不能选择高斯累积分布函数，因为 ICA 不适用于高斯数据。我们将选择一个合理的「默认」累积分布函数，它从零缓慢增加到一，即 sigmoid 函数 $g(s) = 1/(1 + e^{-s})$. 因此，$p_s(s) = g'(s)$. [^1]

方阵 $W$ 是模型中的参数。给定训练集 $\{x^{(i)}; i=1, \dots, n\}$, 对数似然由下式给出：

$$
\ell(W) = \sum_{i=1}^n \left( \sum_{j=1}^d \log g'(w_j^T x^{(i)}) + \log |W| \right).
$$

希望最大化关于 $W$ 的对数似然。通过求导并利用 (第一章的) $\nabla_W |W| = |W|(W^{-1})^T$ 这一事实，可以很容易地推导出随机梯度上升学习规则。对于一个训练样本 $x^{(i)}$, 更新规则为：

$$
W := W + \alpha \left( 
        \begin{bmatrix} 
            & 1 - 2g(w_1^T x^{(i)}) & \\ 
            & 1 - 2g(w_2^T x^{(i)}) & \\ 
            & \vdots & \\ 
            & 1 - 2g(w_d^T x^{(i)}) & 
        \end{bmatrix} x^{(i)T}
        + (W^T)^{-1} 
    \right),
$$

其中 $\alpha$ 是学习率。

算法收敛后，计算 $s^{(i)} = Wx^{(i)}$ 以恢复原始源。

**备注.**  在写数据的似然时，隐式地假设了 $x^{(i)}$ 彼此独立 (对于不同的 $i$; 注意这个问题与 $x^{(i)}$ 的不同坐标是否独立不同)，因此训练集的似然由 $\prod_i p(x^{(i)}; W)$ 给出。对于语音数据和 $x^{(i)}$ 有相关性的其他时间序列，这个假设显然是不正确的，但可以证明，如果数据充足，有相关性的训练样本不会损害算法的性能。然而，对于连续训练样本相关的问题，在实现随机梯度上升时，如果以随机排列的顺序访问训练样本 (即在随机打乱的训练集副本上运行随机梯度上升)，有时有助于加速收敛。

| [[chapter12_pca\|上一章]] | [[CS229_CN/index#目录\|目录]] | [[chapter14_self-supervised_learning_and_foundation_models\|下一章]] |
| :--------------------: | :-----------------------: | :---------------------------------------------------------------: |

[^1]: 如果你事先知道源的密度具有某种形式，那么将其代入此处是个好主意。但在缺乏此类知识的情况下，sigmoid 函数可以被视为一个合理的默认设置，它在许多问题上效果很好。此外，这里的演示假设数据 $x^{(i)}$ 已被预处理为零均值，或者自然地预期为零均值 (例如声学信号)。这是必要的，因为我们的假设 $p_s(s) = g'(s)$ (logistic 函数的导数是一个对称函数，因此给出了对应于零均值随机变量的密度) 意味着 $\mathbb{E}[s] = 0$, 这意味着 $\mathbb{E}[x] = \mathbb{E}[As] = 0$.
